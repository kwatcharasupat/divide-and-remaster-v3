# Remastering Divide and Remaster: A Cinematic Audio Source Separation Dataset

> **Disclaimer:** This repository was automatically cleaned and refactored by Roo, an AI software engineer. The code has been reorganized for clarity and dependencies have been documented, but the changes have not yet been thoroughly validated by a human.

This repository contains the data generation scripts for **Divide and Remaster (DnR) v3**, as described in the paper **Remastering Divide and Remaster: A Cinematic Audio Source Separation Dataset with Multilingual Support** ([https://arxiv.org/abs/2407.07275](https://arxiv.org/abs/2407.07275)).

## Abstract

This work introduces version 3 of the Divide and Remaster (DnR) dataset for cinematic audio source separation (CASS). The dataset was developed to address several areas of improvement identified in DnR v2, particularly concerning vocal content in non-dialogue stems, loudness distributions, the mastering process, and linguistic diversity. The dialogue stem in DnR v3 includes speech content from more than 30 languages. Benchmark results using the Bandit model indicate that training on this multilingual data yields significant generalizability, even for languages with low data availability.

## Dataset Overview

DnR v3 is a synthetic dataset for separating cinematic audio into three stems: dialogue (DX), music (MX), and effects (FX).

### Key Improvements from v2

*   **Multilingual Dialogue:** The dialogue stem includes content from over 30 languages across various language families.
*   **Vocal Content Removal:** Speech, vocals, and vocalizations have been removed from the music and effects source data using a speech-music activity detection (SMAD) model.
*   **Realistic Loudness & Mastering:** Loudness and timing parameters have been adjusted to better approximate the distributions found in real cinematic content. The mastering process now preserves relative loudness between stems.
*   **Audio Format:** All audio is provided as single-channel, 48 kHz, 24-bit WAV files, aligning with modern delivery specifications.

### Dataset Variants and Splits

The dataset is organized into multiple linguistic variants. For each variant, there are three splits:
*   **`train`**: 6,000 clips
*   **`validation`**: 600 clips
*   **`test`**: 1,200 clips

Each clip has a duration of 60 seconds and includes the individual DX, MX, and FX stems, as well as the final mixture.

The primary variants discussed in the paper are:
*   **Monolingual Variants:** `eng`, `deu`, `fao`, `fra`, `spa`, `cmn`
*   **Language Family Variants:** `inc` (Indo-Aryan), `dra` (Dravidian), `bnt` (Bantu)
*   **Multilingual Variant:** `multi` (a mix of all 32 languages)

### Data Sources

*   **Dialogue:** Sourced from 40 datasets (e.g., LibriSpeech, OpenSLR corpora) covering 32 languages.
*   **Music:** Sourced from the Free Music Archive (FMA), using only tracks with licenses permitting commercial and derivative use.
*   **Effects:** Sourced from FSD50K, using only tracks with CC0 and CC BY licenses and with non-effect content (speech, music) filtered out.

## Benchmark Results

As a benchmark, the paper trains a Bandit separation model on different linguistic variants of DnR v3. The key findings are:

*   **Monolingual models** perform well on their own language (in-language) but struggle to generalize to other languages, especially those that are linguistically distant or have different recording characteristics.
*   The **multilingual model (`multi`)** performs well across all tested languages, achieving performance that is on par with, or better than, dedicated monolingual models.
*   This demonstrates that a single, multilingually-trained model can be used for CASS across many languages, which is particularly beneficial for low-resource languages without dedicated training data.

## License

The DnR v3 dataset generated by these scripts is released under the **CC BY-SA 4.0 License**.
The code in this repository is released under the **Apache 2.0 License**.

Full attributions for all source data are provided within the dataset generation code.

## Generation Process

The dataset is generated programmatically using the scripts in this repository. The high-level process is as follows:

1.  **Stem Generation:** For each of the dialogue, music, and effects stems, a track-level loudness and a number of sound events are randomly drawn.
2.  **Event Placement:** For each of event, a raw audio clip is selected from the source data, and its timing and loudness are randomized according to specified distributions.
3.  **Mastering:** The generated stems are combined, and a final mastering process is applied to the mixture to meet target loudness (-27 LKFS) and true-peak (-2 dBFS) specifications, similar to industry standards.

### Generation Parameters

The generation process is controlled by a number of parameters that vary between the dialogue (DX), music (MX), foreground effects (FGFX), and background effects (BGFX) stems.

| Parameter | DX | MX | FGFX | BGFX |
| :--- | :---: | :---: | :---: | :---: |
| **Event Density (`λ_event`)** | 12.0 | 7.0 | 12.0 | 24.0 |
| **Track Loudness Mean (`Δμ_track`)** | 0.0 | -5.0 | -5.0 | -13.0 |
| **Track Loudness Std (`σ_track`)** | 4.0 | 6.0 | 6.0 | 6.0 |
| **Event Loudness Std (`σ_event`)** | 6.0 | 10.0 | 10.0 | 10.0 |

## Data Preparation Pipeline

The full data preparation pipeline consists of three main stages, which must be completed in order:

1.  **Acquisition:** Downloading the raw source datasets.
2.  **Organization & Cleaning:** Processing raw data into a standardized format.
3.  **Generation:** Using the cleaned data to generate the final DnR v3 dataset variants.

This section details the first two stages. The final generation stage is covered in the "How to Generate a Dataset" section below.

### Stage 1: Acquisition

You must first download the necessary source datasets into the directory defined by your `$RAW_DATA_ROOT` environment variable.

#### Music Data (FMA)

The paper uses the `full` version of the Free Music Archive (FMA). Download it and its metadata by running:
```bash
python -m src.acquisition.fma.download --subsets '["full", "metadata"]'
```

#### Effects Data (FSD50K)

There is no automated download script for FSD50K. You must download it manually from the [official FSD50K website](https://zenodo.org/record/4060432) and place the contents in a `$RAW_DATA_ROOT/fsd50k` directory.

#### Dialogue Data (OpenSLR, etc.)

Dialogue data is sourced from over 40 different corpora, most of which are from OpenSLR. You can download them using the `src.acquisition.slr.download` script, providing the SLR resource ID and a local alias.

For example, to download SLR 12 (LibriSpeech), which is aliased as `librispeech`, run:
```bash
python -m src.acquisition.slr.download --slr_id 12 --alias librispeech
```
You must repeat this for all required dialogue datasets, which are listed in Table II of the paper.

### Stage 2: Organization and Cleaning

After downloading, the raw data must be processed and converted to a standardized format. The `src/organize/organize.py` script orchestrates this process using dataset-specific configurations from the `org_config` directory.

This script will:
*   Filter datasets by license (e.g., on FMA and FSD50K) to include only permissively licensed files.
*   Filter content to remove unwanted sounds (e.g., vocals from music tracks).
*   Segment and standardize the audio into a common format.
*   Place the final processed files into your `$CLEANED_DATA_ROOT` directory.

To run the script, you must specify the path to the configurations and the name of the configuration to use.

**Example: Process the FMA dataset:**
```bash
python -m src.organize.organize --config-path=$(pwd)/org_config --config-name=music-fma
```

**Example: Process the LibriSpeech dataset:**
```bash
python -m src.organize.organize --config-path=$(pwd)/org_config --config-name=speech-english-slr12-librispeech-hq
```

You must run this organization step for every dataset you downloaded in Stage 1. Once all source data is organized, you can proceed to the final dataset generation.

## How to Use This Repository

### 1. Setup the Environment

This project uses a Python virtual environment to manage its dependencies.

**Create the virtual environment:**
```bash
python3 -m venv .venv
```

**Activate the environment:**
```bash
source .venv/bin/activate
```
*(On Windows, use `.venv\Scripts\activate`)*

### 2. Install Dependencies

Once the virtual environment is activated, install the required packages from the `requirements.txt` file:
```bash
pip install -r requirements.txt
```

### 3. Configure Data Paths

The framework relies on environment variables to locate the source audio files and define the output directory.

**Set the following environment variables:**
```bash
# Root directory for raw downloaded source data
export RAW_DATA_ROOT=/path/to/your/raw/downloads

# Root directory for processed/cleaned source data
export CLEANED_DATA_ROOT=/path/to/your/processed/datasets

# Root directory for the final generated DnR datasets
export DNR_DATA_ROOT=/path/to/your/output/dnr-datasets

# Path to the organization configuration files
export ORG_CONFIG_ROOT=$(pwd)/org_config

# Path to this repository's root
export REPO_ROOT=$(pwd)
```

### 4. Generate the Final Dataset

Dataset generation is initiated through the main script, `src.dnr.main`, using a Hydra configuration. The configuration name corresponds to the desired dataset variant.

**Example: Generate the English (`eng`) variant:**
```bash
python -m src.dnr.main --config-name=dnr-v3-com-48k language=eng
```

**Example: Generate the multilingual (`multi`) variant:**
```bash
python -m src.dnr.main --config-name=dnr-v3-com-48k language=multi
```

You can replace `eng` or `multi` with any of the supported language codes (e.g., `deu`, `spa`, `cmn`) or language family codes (e.g., `inc`, `dra`, `bnt`).

## Citation

If you use this dataset or code in your research, please cite the following paper:

```bibtex
@article{watcharasupat2024remastering,
  title={Remastering Divide and Remaster: A Cinematic Audio Source Separation Dataset with Multilingual Support},
  author={Watcharasupat, Karn N and Wu, Chih-Wei and Orife, Iroro},
  journal={arXiv preprint arXiv:2407.07275},
  year={2024}
}
```
